{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8fea7b",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39402c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorly.decomposition import tucker, constrained_parafac\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import f1_score, classification_report, roc_auc_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "\n",
    "from SSLH_inference import *\n",
    "from SSLH_utils import *\n",
    "\n",
    "from tensorly.contrib.sparse import decomposition\n",
    "import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c75df5b",
   "metadata": {},
   "source": [
    "### Load Network & Ego Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba4c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(path):\n",
    "    \n",
    "    try:\n",
    "        data = scipy.io.loadmat(path)\n",
    "    except:\n",
    "        print('Invalid data path')\n",
    "\n",
    "    G = nx.from_scipy_sparse_array(data[\"Network\"])\n",
    "    # nx.set_node_attributes(G, bc_data[\"Attributes\"], 'Attributes')\n",
    "    print(str(G))\n",
    "\n",
    "    # convert list of lists to list\n",
    "    labels = [j for i in data[\"Label\"] for j in i]\n",
    "\n",
    "    # Add labels to each node\n",
    "    for i in range(len(G.nodes)):\n",
    "        G.nodes[i]['Anomaly'] = labels[i]\n",
    "\n",
    "    is_undirected = not nx.is_directed(G)\n",
    "\n",
    "    # G = max((G.subgraph(c) for c in nx.connected_components(G)), key=len)\n",
    "    # G = nx.convert_node_labels_to_integers(G)\n",
    "\n",
    "    ego_gs, roots = [], []\n",
    "\n",
    "    # if 0-degree node(s), remove label(s) from consideration\n",
    "    if len(labels) != G.number_of_nodes():\n",
    "        labels = list(nx.get_node_attributes(G, 'Anomaly').values())\n",
    "\n",
    "    for i in tqdm(range(G.number_of_nodes())):\n",
    "        roots.append(G.nodes[i]['Anomaly'])\n",
    "        G_ego = nx.ego_graph(G, i, radius=1, undirected=is_undirected)\n",
    "        if G_ego.number_of_nodes() >= 2:\n",
    "            ego_gs.append(G_ego)\n",
    "\n",
    "    return G, ego_gs, roots, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ad17c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 5196 nodes and 172897 edges\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a67854d5a8248cfa8e7b5da8932c21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load = input('Load previous loaded network? (y/n): ')\n",
    "if load.lower()[0] == 'n':\n",
    "    path = input('Enter file name to save as: ')\n",
    "    \n",
    "    result = load_network(input('Enter dataset/network path: '))\n",
    "\n",
    "    saved_model = open(path, 'wb')\n",
    "    pickle.dump(result, saved_model)\n",
    "    saved_model.close()\n",
    "else:\n",
    "    with open(input('Enter file path: '), 'rb') as f:\n",
    "        G, ego_gs, roots, labels = pickle.load(f)\n",
    "        f.close()\n",
    "        roots = [int(r) for r in roots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a9b1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5196 egonets\n"
     ]
    }
   ],
   "source": [
    "print(f'Using {len(ego_gs)} egonets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccda7c",
   "metadata": {},
   "source": [
    "### Sparse Tensor Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "196f8ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "234e53dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb5702ec9104cd7bb957a0d89a1aa43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "values, indices = [], []\n",
    "padded_gs = []\n",
    "\n",
    "undirected = not nx.is_directed(G)\n",
    "\n",
    "for i, g in enumerate(tqdm(ego_gs)):\n",
    "    ego_adj_list = dict(g.adjacency())\n",
    "    \n",
    "    result = np.zeros((N, N))\n",
    "    for node in ego_adj_list.keys():    \n",
    "        for neighbor in ego_adj_list[node].keys():\n",
    "\n",
    "            result[node][neighbor] = 1\n",
    "            if undirected:\n",
    "               result[neighbor][node] = 1\n",
    "            indices.append([i, node, neighbor])\n",
    "            indices.append([i, neighbor, node])\n",
    "            \n",
    "    norm = np.linalg.norm(result, ord='fro')\n",
    "    values.append((g.number_of_edges(), norm))\n",
    "    padded_gs.append(result * (1/norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.tensor(list(zip(*indices)))\n",
    "# values = torch.ones(len(indices))\n",
    "ten_values = torch.tensor(values)\n",
    "\n",
    "cube = sparse.COO(i, data=ten_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = open('bc_sparse_tensor.sav', 'wb')\n",
    "pickle.dump(cube, saved_model)\n",
    "saved_model.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce5f659",
   "metadata": {},
   "source": [
    "### Tensor Decomposition + Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "948955d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = [int(r) for r in input('Enter ranks, space separated: ').split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca44bcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USING RANK 10\n",
      "\n",
      "\n",
      "Calculating Reconstruction Errors...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc86e4e3a424cf68f3cd75d04fefe98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for rank in ranks:\n",
    "    print(f'\\nUSING RANK {rank}\\n')\n",
    "    load = input('Load Reconstruction Errors? (y/n): ')\n",
    "    # not loading previously calculated reconstruction errors\n",
    "    if load.lower()[0] == 'n':\n",
    "\n",
    "        # checking for valid input\n",
    "        load = input('\\nLoad Previous Decomposition? (y/n): ')\n",
    "        while (load.lower()[0] != 'n' and load.lower()[0] != 'y'):\n",
    "            print('Invalid Input!')\n",
    "            load = input('Load Previous Decomposition? (y/n): ')\n",
    "        decomp = input('Select Tucker (1) or CP (2) Decomposition: ')\n",
    "        while (decomp != '1' and decomp != '2'):\n",
    "            print('Invalid Input!')\n",
    "            decomp = input('Select Tucker (1) or CP (2) Decomposition: ')\n",
    "\n",
    "        if load.lower()[0] == 'n':\n",
    "            path = input('Enter file name to save factors as: ')\n",
    "            if decomp == '1':\n",
    "                print('Tucker Decomposition...')\n",
    "                _, factors = decomposition.tucker(cube, rank=rank, init='random')\n",
    "            elif decomp == '2':\n",
    "                print('Parafac Decomposition...')\n",
    "                _, factors = decomposition.parafac(cube, rank=rank, init='random')\n",
    "            print(f\"Factors Saved to {path}\\n\")\n",
    "            saved_model = open(path, 'wb')\n",
    "            pickle.dump(factors, saved_model)\n",
    "            saved_model.close()\n",
    "        else:\n",
    "            with open(input('Enter file path: '), 'rb') as f:\n",
    "                factors = pickle.load(f)\n",
    "                f.close()\n",
    "                print()\n",
    "        \n",
    "        A, B, C = factors\n",
    "        if decomp == '1':\n",
    "            A, B, C, = np.array(A), np.array(B), np.array(C)\n",
    "        elif decomp == '2':\n",
    "            A, B, C = A.todense(), B.todense(), C.todense()\n",
    "            \n",
    "\n",
    "        errors = []\n",
    "        print(\"Calculating Reconstruction Errors...\")\n",
    "        for gs in tqdm(padded_gs):\n",
    "            if decomp == '1':\n",
    "                gs_p = (A @ ((A.T @ gs) @ B) @ B.T)\n",
    "            elif decomp == '2':\n",
    "                gs_p = (A @ ((np.linalg.pinv(A) @ gs) @ B) @ np.linalg.pinv(B))\n",
    "            d = np.linalg.norm(gs - gs_p, ord='fro')\n",
    "            errors.append(d)\n",
    "\n",
    "        errors = np.array(errors).reshape(-1, 1)\n",
    "\n",
    "        path = input('Enter file name to save reconstruction errors: ')\n",
    "        saved_model = open(path, 'wb')\n",
    "        pickle.dump(errors, saved_model)\n",
    "        saved_model.close()\n",
    "        print()\n",
    "\n",
    "    # loading previously calculated reconstruction errors\n",
    "    else:\n",
    "        with open(input('Enter file path: '), 'rb') as f:\n",
    "            errors = pickle.load(f)\n",
    "            f.close()    \n",
    "            print()    \n",
    "\n",
    "    scale = MinMaxScaler()\n",
    "    embeddings = scale.fit_transform(np.array(errors))\n",
    "\n",
    "    scores.append(('No Model', rank, roc_auc_score(labels, embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1394657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, auc in scores:\n",
    "    print(f'Model: {name}, AUC score: {auc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('anom_detect')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "13b2b21ceeb0d6cdc283bf1c7ddabad54f6fedb13073061a08677843d5250697"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
