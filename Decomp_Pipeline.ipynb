{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8fea7b",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39402c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorly.decomposition import tucker, constrained_parafac\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "\n",
    "from SSLH_inference import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c75df5b",
   "metadata": {},
   "source": [
    "### Load Network & Ego Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba4c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(path):\n",
    "    \n",
    "    try:\n",
    "        data = scipy.io.loadmat(path)\n",
    "    except:\n",
    "        print('Invalid data path')\n",
    "\n",
    "    G = nx.from_scipy_sparse_array(data[\"Network\"])\n",
    "    # nx.set_node_attributes(G, bc_data[\"Attributes\"], 'Attributes')\n",
    "    print(str(G))\n",
    "\n",
    "    # convert list of lists to list\n",
    "    labels = [j for i in data[\"Label\"] for j in i]\n",
    "\n",
    "    # Add labels to each node\n",
    "    for i in range(len(G.nodes)):\n",
    "        G.nodes[i]['Anomaly'] = labels[i]\n",
    "\n",
    "    G = max((G.subgraph(c) for c in nx.connected_components(G)), key=len)\n",
    "    G = nx.convert_node_labels_to_integers(G)\n",
    "    ego_gs, roots = [], []\n",
    "\n",
    "    for i in tqdm(range(G.number_of_nodes())):\n",
    "        roots.append(G.nodes[i]['Anomaly'])\n",
    "        G_ego = nx.ego_graph(G, i, radius=1)\n",
    "        if G_ego.number_of_nodes() >= 2:\n",
    "            ego_gs.append(G_ego)\n",
    "\n",
    "    return G, ego_gs, roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1251ca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter dataset path: datasets/blogcatalog.mat\n",
      "Graph with 5196 nodes and 172897 edges\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eda5a58abcb47128b496512582c9116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G, ego_gs, roots = load_network(input('Enter dataset path: '))\n",
    "\n",
    "roots = [int(r) for r in roots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a9b1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5196 egonets\n"
     ]
    }
   ],
   "source": [
    "print(f'Using {len(ego_gs)} egonets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d288b469",
   "metadata": {},
   "source": [
    "### Separate Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1359c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: anomalous egonet ==> anomalous root\n",
      "2: anomalous egonet ==> any anomalous node\n",
      "Anomalous definition: 2\n"
     ]
    }
   ],
   "source": [
    "# 1: anomalous egonet ==> anomalous root\n",
    "# 2: anomalous egonet ==> any anomalous node\n",
    "\n",
    "print('1: anomalous egonet ==> anomalous root')\n",
    "print('2: anomalous egonet ==> any anomalous node')\n",
    "\n",
    "anom_def = int(input('Anomalous definition: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4481a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get anomalous egonets for definition 2\n",
    "def is_anomolous(G):\n",
    "    for node in G.nodes():\n",
    "        if G.nodes[node]['Anomaly'] == 1:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab047b07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anom_def' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m anom_gs, clean_gs \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43manom_def\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, root_attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(roots)):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m root_attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'anom_def' is not defined"
     ]
    }
   ],
   "source": [
    "anom_gs, clean_gs = [], []\n",
    "if anom_def == 1:\n",
    "    for idx, root_attr in enumerate(tqdm(roots)):\n",
    "        if root_attr == 1:\n",
    "            anom_gs.append(ego_gs[idx])\n",
    "        else:\n",
    "            clean_gs.append(ego_gs[idx])\n",
    "elif anom_def == 2:\n",
    "    for idx, g in enumerate(tqdm(ego_gs)):\n",
    "        # check if root is anomolous first\n",
    "        if is_anomolous(g):\n",
    "            anom_gs.append(g)\n",
    "        else:\n",
    "            clean_gs.append(g)\n",
    "        \n",
    "print(f'# of anomalous graphs: {len(anom_gs)}/{len(ego_gs)}')\n",
    "print(f'# of clean graphs: {len(clean_gs)}/{len(ego_gs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d782d",
   "metadata": {},
   "source": [
    "### Build Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d429dba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input a slice size for tensor: 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6858ea283a9e4c5ab7fb394123235fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "slice_size = int(input(\"Input a slice size for tensor: \"))\n",
    "\n",
    "cube = torch.empty((1, slice_size, slice_size, slice_size))\n",
    "\n",
    "for i in tqdm(range(len(ego_gs[:slice_size]))):\n",
    "    slice_gs = nx.to_numpy_array(ego_gs[i])[:slice_size, :slice_size]\n",
    "    result = np.zeros((slice_size, slice_size))\n",
    "    result[:slice_gs.shape[0],:slice_gs.shape[1]] = slice_gs\n",
    "    \n",
    "    ten = torch.as_tensor(result)\n",
    "    dim = ten.shape[0]\n",
    "    cube[0, :dim, :dim, i] = ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73fbb237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec7caec636843038fedac05cf32d5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# padding adjacency matrices\n",
    "\n",
    "# padded_anom = []\n",
    "# for gs in tqdm(anom_gs):\n",
    "#     g = nx.to_numpy_array(gs)\n",
    "#     padded = np.zeros((slice_size, slice_size))\n",
    "#     if len(padded) >= len(g):\n",
    "#         padded[:g.shape[0], :g.shape[1]] = g\n",
    "#     else:\n",
    "#         padded = g[:slice_size, :slice_size]\n",
    "#     padded_anom.append(padded)\n",
    "    \n",
    "# padded_clean = []\n",
    "# for gs in tqdm(clean_gs):\n",
    "#     g = nx.to_numpy_array(gs)\n",
    "#     padded = np.zeros((slice_size, slice_size))\n",
    "#     if len(padded) >= len(g):\n",
    "#         padded[:g.shape[0], :g.shape[1]] = g\n",
    "#     else:\n",
    "#         padded = g[:slice_size, :slice_size]\n",
    "#     padded_clean.append(padded)\n",
    "\n",
    "padded_gs = []\n",
    "for gs in tqdm(ego_gs):\n",
    "    g = nx.to_numpy_array(gs)\n",
    "    padded = np.zeros((slice_size, slice_size))\n",
    "    if len(padded) >= len(g):\n",
    "        padded[:g.shape[0], :g.shape[1]] = g\n",
    "    else:\n",
    "        padded = g[:slice_size, :slice_size]\n",
    "    padded_gs.append(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0841b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anom_y = np.zeros(len(anom_gs))\n",
    "# clean_y = np.zeros(len(clean_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce5f659",
   "metadata": {},
   "source": [
    "### Tensor Decomposition + Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "948955d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter ranks, space separated: 10\n"
     ]
    }
   ],
   "source": [
    "ranks = [int(r) for r in input('Enter ranks, space separated: ').split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44bcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load previous decomposition? (y/n): n\n",
      "USING RANK 10\n",
      "\n",
      "Enter file name to save as: bc_tkd_r10\n",
      "Select Tucker (1) or CP (2) decomposition: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tutt/miniconda3/envs/anom_detect/lib/python3.10/site-packages/tensorly/tucker_tensor.py:380: RuntimeWarning: Given only one int for 'rank' for decomposition a tensor of order 3. Using this rank for all modes.\n",
      "  warnings.warn(message, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "for rank in ranks:\n",
    "    load = input('Load previous decomposition? (y/n): ')\n",
    "    if load.lower()[0] == 'n':\n",
    "        print(f'USING RANK {rank}\\n')\n",
    "        path = input('Enter file name to save as: ') + '.sav'\n",
    "        decomp = input('Select Tucker (1) or CP (2) decomposition: ')\n",
    "        if decomp == '1':\n",
    "            core, factors = tucker(cube[0].numpy(), rank=rank)\n",
    "        elif decomp == '2':\n",
    "            weights, factors = constrained_parafac(cube[0].numpy(), rank=rank)\n",
    "        saved_model = open(path, 'wb')\n",
    "        pickle.dump(factors, saved_model)\n",
    "        saved_model.close()\n",
    "    else:\n",
    "        with open(input('Enter file path: '), 'rb') as f:\n",
    "            factors = pickle.load(f)\n",
    "            f.close()\n",
    "    \n",
    "    A, B, C = factors\n",
    "    A, B, C, = np.array(A), np.array(B), np.array(C)\n",
    "    \n",
    "#     results_anom = []\n",
    "#     for gs in tqdm(padded_anom):\n",
    "#         gs_p = (A @ ((A.T @ gs) @ B) @ B.T)\n",
    "#         d = np.linalg.norm(gs - gs_p)\n",
    "#         results_anom.append(d)\n",
    "        \n",
    "#     results_clean = []\n",
    "#     for gs in tqdm(padded_clean):\n",
    "#         gs_p = (A @ ((A.T @ gs) @ B) @ B.T)\n",
    "#         d = np.linalg.norm(gs - gs_p)\n",
    "#         results_clean.append(d)\n",
    "\n",
    "\n",
    "    errors = []\n",
    "    for gs in tqdm(padded_gs):\n",
    "        gs_p = (A @ ((A.T @ gs) @ B) @ B.T)\n",
    "        d = np.linalg.norm(gs - gs_p)\n",
    "        errors.append(d)\n",
    "\n",
    "\n",
    "    ## BELIEF PROPOGATION\n",
    "\n",
    "    errors = errors.reshape(-1, 1)\n",
    "    \n",
    "    # adjusted for homophily\n",
    "    H = np.flip(create_parameterized_H(2, 8, symmetric=True), axis=1)\n",
    "    W = csr_matrix(np.ones((len(scores), len(scores))))\n",
    "    X = scores\n",
    "    \n",
    "    out = linBP_symmetric(X, W, H)\n",
    "    \n",
    "    scale = MinMaxScaler()\n",
    "    embeddings = scale.fit_transform(np.array(out))\n",
    "    \n",
    "    roc_auc_score(labels, embeddings[:, 1])\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.displot(out[labels])\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.displot(out[~np.array(labels)])\n",
    "    \n",
    "    ## CLUSTERING\n",
    "    \n",
    "#     clus_method = input('Enter clustering method (1) Isolation Forest or (2) K-Means: ')\n",
    "        \n",
    "#     ## PLOTS\n",
    "    \n",
    "#     plt.figure(figsize=(15,5))\n",
    "#     plt.title('Reconstruction Error')\n",
    "#     if len(anom_y) > len(clean_y):\n",
    "#         sns.scatterplot(x=results_anom, y=anom_y, marker='x')\n",
    "#         sns.scatterplot(x=results_clean, y=clean_y, marker='o')\n",
    "#         plt.scatter([np.median(results_anom)], 0, c='red')\n",
    "#         plt.scatter([np.median(results_clean)], 0, c='blue')\n",
    "#     else:\n",
    "#         sns.scatterplot(x=results_clean, y=clean_y, marker='o')  \n",
    "#         sns.scatterplot(x=results_anom, y=anom_y, marker='x')\n",
    "#         plt.scatter([np.median(results_anom)], 0, c='red')\n",
    "#         plt.scatter([np.median(results_clean)], 0, c='blue')\n",
    "\n",
    "#     plt.legend(['Anom', 'Clean'])\n",
    "    \n",
    "#     plt.figure()\n",
    "#     sns.scatterplot(x=results_anom, y=[0] * len(anom_y), marker='x')\n",
    "#     sns.scatterplot(x=results_clean, y=[1] * len(clean_y), marker='o')\n",
    "#     plt.scatter([np.median(results_anom)], 0, c='red')\n",
    "#     plt.scatter([np.median(results_clean)], 1, c='blue')\n",
    "    \n",
    "#     plt.figure(figsize=(15,5))\n",
    "#     plt.title('Reconstruction Error')\n",
    "#     sns.histplot(results_anom)\n",
    "#     sns.histplot(results_clean, ax=plt.gca())\n",
    "    \n",
    "#     plt.figure(figsize=(15,5))\n",
    "#     plt.title('Reconstruction Error')\n",
    "#     comb = [results_clean, results_anom]\n",
    "#     plt.boxplot(comb)\n",
    "    \n",
    "    \n",
    "#     ## KMEANS - 2 clusters\n",
    "    \n",
    "#     results, true_labels = [], []\n",
    "#     results.extend(results_clean); results.extend(results_anom)\n",
    "#     true_labels.extend([0] * len(results_clean)); true_labels.extend([1] * len(results_anom))\n",
    "    \n",
    "#     results = np.array(results).reshape(-1, 1)\n",
    "        \n",
    "#     if clus_method == '1':\n",
    "#         ifor = IsolationForest()\n",
    "#         pred = ifor.fit_predict(results)\n",
    "#         labels = [1 if p == -1 else 0 for p in pred]\n",
    "#         plt.title('IF Clustering')\n",
    "#         sns.scatterplot(x=results.flatten(), y=np.zeros(len(results)), hue=labels)\n",
    "#         plt.scatter([np.median(results_anom)], 0, c='red')\n",
    "#         plt.scatter([np.median(results_clean)], 0, c='blue')\n",
    "#     elif clus_method == '2':\n",
    "#         plt.figure(figsize=(15,5))\n",
    "#         plt.title('K-Means')\n",
    "#         kmeans = KMeans(n_clusters=2, random_state=1).fit(results)\n",
    "#         sns.scatterplot(x=results.flatten(), y=np.zeros(len(results)), hue=kmeans.labels_)\n",
    "#         plt.scatter([np.median(results_anom)], 0, c='red')\n",
    "#         plt.scatter([np.median(results_clean)], 0, c='blue')\n",
    "#         labels = kmeans.labels_\n",
    "    \n",
    "#     print(np.unique(true_labels, return_counts=True))\n",
    "#     print(np.unique(labels, return_counts=True))\n",
    "\n",
    "#     print(classification_report(true_labels, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1394657e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
