{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "from itertools import combinations as comb\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from numpy.linalg import pinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(path):\n",
    "    \n",
    "    try:\n",
    "        data = scipy.io.loadmat(path)\n",
    "    except:\n",
    "        print('Invalid data path')\n",
    "\n",
    "    G = nx.from_scipy_sparse_array(data[\"Network\"])\n",
    "    # nx.set_node_attributes(G, bc_data[\"Attributes\"], 'Attributes')\n",
    "    print(str(G))\n",
    "\n",
    "    # convert list of lists to list\n",
    "    labels = [j for i in data[\"Label\"] for j in i]\n",
    "\n",
    "    # Add labels to each node\n",
    "    for i in range(len(G.nodes)):\n",
    "        G.nodes[i]['Anomaly'] = labels[i]\n",
    "\n",
    "    G = max((G.subgraph(c) for c in nx.connected_components(G)), key=len)\n",
    "    G = nx.convert_node_labels_to_integers(G)\n",
    "    ego_gs, roots = [], []\n",
    "\n",
    "    for i in tqdm(range(G.number_of_nodes())):\n",
    "        roots.append(G.nodes[i]['Anomaly'])\n",
    "        G_ego = nx.ego_graph(G, i, radius=1)\n",
    "        if G_ego.number_of_nodes() >= 2:\n",
    "            ego_gs.append(G_ego)\n",
    "\n",
    "    return G, ego_gs, roots\n",
    "\n",
    "# get anomalous egonets for definition 2\n",
    "def is_anomolous(G):\n",
    "    for node in G.nodes():\n",
    "        if G.nodes[node]['Anomaly'] == 1:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4da746",
   "metadata": {},
   "outputs": [],
   "source": [
    "G, ego_gs, roots = load_network('datasets/BlogCatalog.mat')\n",
    "\n",
    "roots = [int(r) for r in roots]\n",
    "\n",
    "print(f'Using {len(ego_gs)} egonets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b91b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: anomalous egonet ==> anomalous root\n",
    "# 2: anomalous egonet ==> any anomalous node\n",
    "\n",
    "print('1: anomalous egonet ==> anomalous root')\n",
    "print('2: anomalous egonet ==> any anomalous node')\n",
    "\n",
    "anom_def = int(input('Anomalous definition: '))\n",
    "\n",
    "anom_gs, clean_gs = [], []\n",
    "if anom_def == 1:\n",
    "    for idx, root_attr in enumerate(tqdm(roots)):\n",
    "        if root_attr == 1:\n",
    "            anom_gs.append(ego_gs[idx])\n",
    "        else:\n",
    "            clean_gs.append(ego_gs[idx])\n",
    "elif anom_def == 2:\n",
    "    for idx, g in enumerate(tqdm(ego_gs)):\n",
    "        # check if root is anomolous first\n",
    "        if is_anomolous(g):\n",
    "            anom_gs.append(g)\n",
    "        else:\n",
    "            clean_gs.append(g)\n",
    "        \n",
    "print(f'# of anomalous graphs: {len(anom_gs)}/{len(ego_gs)}')\n",
    "print(f'# of clean graphs: {len(clean_gs)}/{len(ego_gs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ee607",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_size = int(input(\"Input a slice size for tensor: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3bd8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = torch.empty((1, slice_size, slice_size, slice_size))\n",
    "\n",
    "for i in tqdm(range(len(clean_gs[:slice_size]))):\n",
    "    slice_gs = nx.to_numpy_array(clean_gs[i])[:slice_size, :slice_size]\n",
    "    result = np.zeros((slice_size, slice_size))\n",
    "    result[:slice_gs.shape[0],:slice_gs.shape[1]] = slice_gs\n",
    "    \n",
    "    ten = torch.as_tensor(result)\n",
    "    dim = ten.shape[0]\n",
    "    cube[0, :dim, :dim, i] = ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding adjacency matrices\n",
    "padded_anom = []\n",
    "for gs in tqdm(anom_gs):\n",
    "    g = nx.to_numpy_array(gs)\n",
    "    padded = np.zeros((slice_size, slice_size))\n",
    "    if len(padded) >= len(g):\n",
    "        padded[:g.shape[0], :g.shape[1]] = g\n",
    "    else:\n",
    "        padded = g[:slice_size, :slice_size]\n",
    "    padded_anom.append(padded)\n",
    "    \n",
    "padded_clean = []\n",
    "for gs in tqdm(clean_gs):\n",
    "    g = nx.to_numpy_array(gs)\n",
    "    padded = np.zeros((slice_size, slice_size))\n",
    "    if len(padded) >= len(g):\n",
    "        padded[:g.shape[0], :g.shape[1]] = g\n",
    "    else:\n",
    "        padded = g[:slice_size, :slice_size]\n",
    "    padded_clean.append(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3292279",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bc_decomp_r10.sav', 'rb') as f:\n",
    "    _, factors = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "A, B, C = factors\n",
    "A, B, C, = np.array(A), np.array(B), np.array(C)\n",
    "\n",
    "results_anom = []\n",
    "for gs in tqdm(padded_anom):\n",
    "    gs_p = (A @ ((A.T @ gs) @ B) @ B.T)\n",
    "    d = np.linalg.norm(gs - gs_p)\n",
    "    results_anom.append(d)\n",
    "\n",
    "results_clean = []\n",
    "for gs in tqdm(padded_clean):\n",
    "    gs_p = (A @ ((A.T @ gs) @ B) @ B.T)\n",
    "    d = np.linalg.norm(gs - gs_p)\n",
    "    results_clean.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6abed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.column_stack((np.array(results_clean + results_anom), np.zeros(len(ego_gs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1708c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "labels +=  [0] * len(results_clean) + [1] * len(results_anom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c09031",
   "metadata": {},
   "source": [
    "# Kitchen Sink\n",
    "https://pyod.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d2f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.suod import SUOD\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.anogan import AnoGAN\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.deep_svdd import DeepSVDD\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF\n",
    "\n",
    "from pyod.utils.data import evaluate_print\n",
    "from pyod.utils.example import visualize\n",
    "\n",
    "import torch\n",
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(results, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f775405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSVDD(), DeepSVDD(use_ae=True), AnoGAN(), AutoEncoder()\n",
    "detector_list = [LOF(n_neighbors=20), IForest(n_estimators=100), CBLOF(), AutoEncoder([2, 1, 1, 2])]\n",
    "\n",
    "# decide the number of parallel process, and the combination method\n",
    "# then clf can be used as any outlier detection model\n",
    "clf = SUOD(base_estimators=detector_list, n_jobs=2, combination='average',\n",
    "           verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e158b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction labels and outlier scores of the training data\n",
    "y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "# get the prediction on the test data\n",
    "y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
    "\n",
    "# evaluate and print the results\n",
    "print(\"\\nOn Training Data:\")\n",
    "evaluate_print('SUOD', y_train, y_train_scores)\n",
    "print(\"\\nOn Test Data:\")\n",
    "evaluate_print('SUOD', y_test, y_test_scores)\n",
    "\n",
    "# visualize the results\n",
    "visualize('SUOD', X_train, y_train, X_test, y_test, y_train_pred,\n",
    "          y_test_pred, show_figure=True, save_figure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84324191",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = open('suod.sav', 'wb')\n",
    "pickle.dump(clf, saved_model)\n",
    "saved_model.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
